{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import json\n",
    "from lxml import etree\n",
    "import os\n",
    "import re\n",
    "from IPython.display import HTML, display\n",
    "import pickle\n",
    "\n",
    "class MySearcherC4:\n",
    "    \"\"\"\n",
    "    第4次课上集成的搜索类\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.news_list = []\n",
    "        self.fetch_data()\n",
    "\n",
    "    def fetch_data(self):\n",
    "        news_list_file = 'news_list.dat'\n",
    "\n",
    "        if os.path.exists(news_list_file):\n",
    "            with open(news_list_file, 'rb') as file:\n",
    "                self.news_list = pickle.load(file)\n",
    "\n",
    "        else:\n",
    "            urls = [\"https://tech.163.com/special/00097UHL/tech_datalist.js?callback=data_callback\", \"https://tech.163.com/special/00097UHL/tech_datalist_02.js?callback=data_callback\", \"https://tech.163.com/special/00097UHL/tech_datalist_03.js?callback=data_callback\"]\n",
    "            headers = CaseInsensitiveDict()\n",
    "            headers[\"Referer\"] = \"https://tech.163.com/\"\n",
    "            headers[\"user-agent\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"\n",
    "\n",
    "            url_set = set()\n",
    "            processed_count = 0\n",
    "\n",
    "            for url in urls:\n",
    "                resp = requests.get(url, headers=headers)\n",
    "                json_data = json.loads(resp.text[len('data_callback('):-1])\n",
    "\n",
    "                for news in json_data:\n",
    "                    title = news['title']\n",
    "                    docurl = news['docurl']\n",
    "\n",
    "                    if docurl not in url_set:\n",
    "                        doc_resp = requests.get(docurl, headers=headers, timeout=30)\n",
    "                        doc_resp.encoding = 'utf-8'\n",
    "                        tree = etree.HTML(doc_resp.text)\n",
    "\n",
    "                        post_body = tree.xpath(\"//div[@class='post_body']\")\n",
    "                        if post_body:\n",
    "                            paragraphs = post_body[0].xpath(\".//p\")\n",
    "                            html = ''.join(etree.tostring(p, method='html', encoding='unicode') for p in paragraphs)\n",
    "                            text = ''.join(t.strip() for t in etree.HTML(html).xpath(\"//text()\") if t.strip())\n",
    "\n",
    "                            self.news_list.append([docurl, title, text])\n",
    "                        \n",
    "                        url_set.add(docurl)\n",
    "\n",
    "                    processed_count += 1\n",
    "                    if processed_count % 15 == 0:\n",
    "                        print(f'{processed_count} processed.')\n",
    "\n",
    "            if self.news_list:\n",
    "                with open(news_list_file, 'wb') as file:\n",
    "                    pickle.dump(self.news_list, file)\n",
    "\n",
    "    def search_keywords(self, keyword):\n",
    "        keyword_lower = keyword.lower()\n",
    "\n",
    "        result = [(i, self.score(item, keyword)) for i, item in enumerate(self.news_list) if (keyword_lower in item[1].lower() or keyword_lower in item[2].lower())]\n",
    "        result.sort(key=lambda x: x[1], reverse=True)\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def highlight(self, text, keyword):\n",
    "        return re.sub(pattern=f'({keyword})', repl=r'<span style=\"color:#dd4b39\">\\1</span>', string=text, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "    def score(self, item, keyword):\n",
    "        keyword_lower = keyword.lower()\n",
    "        return (item[1].lower().count(keyword_lower) * 5 + item[2].lower().count(keyword_lower) * 3)\n",
    "    \n",
    "    def render_search_result(self, keyword):\n",
    "        result = self.search_keywords(keyword)\n",
    "        for item in result:\n",
    "            clickable_title = f'<a href=\"{self.news_list[item[0]][0]}\" target=\"_blank\">{self.highlight(self.news_list[item[0]][1], keyword)}</a>'\n",
    "            display(HTML(f'[{item[1]}] {clickable_title}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "class MySearcherC5V1(MySearcherC4):\n",
    "    \"\"\"\n",
    "    增加scale参数 用于倍增news\n",
    "    \"\"\"\n",
    "    def __init__(self, scale=1):\n",
    "        super().__init__()\n",
    "        self.news_list *= scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 processed.\n",
      "30 processed.\n",
      "45 processed.\n",
      "60 processed.\n",
      "75 processed.\n",
      "90 processed.\n",
      "CPU times: total: 3.41 s\n",
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcher_1x = MySearcherC5V1()\n",
    "searcher_10x = MySearcherC5V1(scale=10)\n",
    "searcher_100x = MySearcherC5V1(scale=100)\n",
    "searcher_1000x = MySearcherC5V1(scale=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 5 ms\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 30.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time r=searcher_1x.search_keywords('ai')\n",
    "%time r=searcher_10x.search_keywords('ai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1x\t 0.003\n",
      "10x\t 0.017\n",
      "100x\t 0.156\n",
      "1000x\t 1.547\n"
     ]
    }
   ],
   "source": [
    "print('1x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcher_1x.search_keywords('ai')\", globals=globals(), number=1))\n",
    "print('10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcher_10x.search_keywords('ai')\", globals=globals(), number=1))\n",
    "print('100x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcher_100x.search_keywords('ai')\", globals=globals(), number=1))\n",
    "print('1000x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcher_1000x.search_keywords('ai')\", globals=globals(), number=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1time10x\t 0.017\n",
      "10times10x\t 0.143\n",
      "100times10x\t 1.463\n",
      "1000times10x\t 14.618\n"
     ]
    }
   ],
   "source": [
    "print('1time10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcher_10x.search_keywords('ai')\", globals=globals(), number=1))\n",
    "print('10times10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcher_10x.search_keywords('ai')\", globals=globals(), number=10))\n",
    "print('100times10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcher_10x.search_keywords('ai')\", globals=globals(), number=100))\n",
    "print('1000times10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcher_10x.search_keywords('ai')\", globals=globals(), number=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySearcherC5V2(MySearcherC5V1):\n",
    "    \"\"\"\n",
    "    增加缓存机制 搜索相同关键词时 无需重新计算\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale=1):\n",
    "        super().__init__(scale)\n",
    "        self.cache = {}\n",
    "\n",
    "    def search_keywords(self, keyword):\n",
    "        keyword_lower = keyword.lower()\n",
    "        if keyword_lower in self.cache:\n",
    "            result = self.cache[keyword_lower]\n",
    "        else:\n",
    "            result = [(i, self.score(item, keyword)) for i, item in enumerate(self.news_list) if (keyword_lower in item[1].lower() or keyword_lower in item[2].lower())]\n",
    "            result.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.cache[keyword_lower] = result\n",
    "        # 单一出口\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 998 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcherV2_10x = MySearcherC5V2(scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1time10x\t 0.036\n",
      "10times10x\t 0.000\n",
      "100times10x\t 0.000\n",
      "1000times10x\t 0.001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('1time10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcherV2_10x.search_keywords('ai')\", globals=globals(), number=1))\n",
    "print('10times10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcherV2_10x.search_keywords('ai')\", globals=globals(), number=10))\n",
    "print('100times10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcherV2_10x.search_keywords('ai')\", globals=globals(), number=100))\n",
    "print('1000times10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcherV2_10x.search_keywords('ai')\", globals=globals(), number=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySearcherC5V3(MySearcherC5V2):\n",
    "    \"\"\"\n",
    "    pseudo查询词 预处理 - 用线下处理 代替线上处理\n",
    "    \"\"\"\n",
    "    def __init__(self, scale=1):\n",
    "        super().__init__(scale)\n",
    "        self.trending_words = set(['ai', '华为', 'iphone'])\n",
    "        self.cache_trending_words()\n",
    "\n",
    "    def cache_trending_words(self):\n",
    "        for word in self.trending_words:\n",
    "            r = self.search_keywords(word)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 62.5 ms\n",
      "Wall time: 43.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcherV3_10x = MySearcherC5V3(scale=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1time10x\t 0.000\n",
      "10times10x\t 0.000\n",
      "100times10x\t 0.000\n",
      "1000times10x\t 0.000\n"
     ]
    }
   ],
   "source": [
    "print('1time10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcherV3_10x.search_keywords('ai')\", globals=globals(), number=1))\n",
    "print('10times10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcherV3_10x.search_keywords('ai')\", globals=globals(), number=10))\n",
    "print('100times10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcherV3_10x.search_keywords('ai')\", globals=globals(), number=100))\n",
    "print('1000times10x\\t', '%0.3f' % timeit.timeit(stmt=\"r=searcherV3_10x.search_keywords('ai')\", globals=globals(), number=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "class MySearcherC5V4(MySearcherC5V2):\n",
    "    \"\"\"\n",
    "    分词得到的词（用文档过滤词库） 预处理\n",
    "    \"\"\"\n",
    "    def __init__(self, scale=1):\n",
    "        super().__init__(scale)\n",
    "        self.cache_trending_words()\n",
    "\n",
    "    def cache_trending_words(self):\n",
    "        for news in self.news_list:\n",
    "            for word in jieba.cut(news[1] + news[2], cut_all=True):\n",
    "                r = self.search_keywords(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.8 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcherV4_1x = MySearcherC5V4()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "r = searcherV4_1x.search_keywords('苹果')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
