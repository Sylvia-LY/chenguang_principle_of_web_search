{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import json\n",
    "from lxml import etree\n",
    "import os\n",
    "import re\n",
    "from IPython.display import HTML, display\n",
    "import pickle\n",
    "import jieba\n",
    "\n",
    "\n",
    "class MySearcherC5:\n",
    "    def __init__(self, scale=1):\n",
    "        self.news_list = []\n",
    "        self.fetch_data()\n",
    "        self.news_list *= scale\n",
    "        self.cache = {}\n",
    "        self.cache_segmented_words()\n",
    "\n",
    "    def fetch_data(self):\n",
    "        news_list_file = 'news_list.dat'\n",
    "\n",
    "        if os.path.exists(news_list_file):\n",
    "            with open(news_list_file, 'rb') as file:\n",
    "                self.news_list = pickle.load(file)\n",
    "\n",
    "        else:\n",
    "            urls = [\"https://tech.163.com/special/00097UHL/tech_datalist.js?callback=data_callback\", \"https://tech.163.com/special/00097UHL/tech_datalist_02.js?callback=data_callback\", \"https://tech.163.com/special/00097UHL/tech_datalist_03.js?callback=data_callback\"]\n",
    "            headers = CaseInsensitiveDict()\n",
    "            headers[\"Referer\"] = \"https://tech.163.com/\"\n",
    "            headers[\"user-agent\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"\n",
    "\n",
    "            url_set = set()\n",
    "            processed_count = 0\n",
    "\n",
    "            for url in urls:\n",
    "                resp = requests.get(url, headers=headers)\n",
    "                json_data = json.loads(resp.text[len('data_callback('):-1])\n",
    "\n",
    "                for news in json_data:\n",
    "                    title = news['title']\n",
    "                    docurl = news['docurl']\n",
    "\n",
    "                    if docurl not in url_set:\n",
    "                        doc_resp = requests.get(docurl, headers=headers, timeout=30)\n",
    "                        doc_resp.encoding = 'utf-8'\n",
    "                        tree = etree.HTML(doc_resp.text)\n",
    "\n",
    "                        post_body = tree.xpath(\"//div[@class='post_body']\")\n",
    "                        if post_body:\n",
    "                            paragraphs = post_body[0].xpath(\".//p\")\n",
    "                            html = ''.join(etree.tostring(p, method='html', encoding='unicode') for p in paragraphs)\n",
    "                            text = ''.join(t.strip() for t in etree.HTML(html).xpath(\"//text()\") if t.strip())\n",
    "\n",
    "                            self.news_list.append([docurl, title, text])\n",
    "                        \n",
    "                        url_set.add(docurl)\n",
    "\n",
    "                    processed_count += 1\n",
    "                    if processed_count % 15 == 0:\n",
    "                        print(f'{processed_count} processed.')\n",
    "\n",
    "            \n",
    "            if self.news_list:\n",
    "                with open(news_list_file, 'wb') as file:\n",
    "                    pickle.dump(self.news_list, file)\n",
    "\n",
    "    def score(self, item, keyword):\n",
    "        keyword_lower = keyword.lower()\n",
    "        return (item[1].lower().count(keyword_lower) * 5 + item[2].lower().count(keyword_lower) * 3)\n",
    "\n",
    "\n",
    "    def search_keywords(self, keyword):\n",
    "        keyword_lower = keyword.lower()\n",
    "        if keyword_lower in self.cache:\n",
    "            result = self.cache[keyword_lower]\n",
    "        else:\n",
    "            result = [(i, self.score(item, keyword)) for i, item in enumerate(self.news_list) if (keyword_lower in item[1].lower() or keyword_lower in item[2].lower())]\n",
    "            result.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.cache[keyword_lower] = result\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def highlight(self, text, keyword):\n",
    "        return re.sub(pattern=f'({keyword})', repl=r'<span style=\"color:#dd4b39\">\\1</span>', string=text, flags=re.IGNORECASE)\n",
    "\n",
    "    def render_search_result(self, keyword):\n",
    "        result = self.search_keywords(keyword)\n",
    "        for item in result:\n",
    "            clickable_title = f'<a href=\"{self.news_list[item[0]][0]}\" target=\"_blank\">{self.highlight(self.news_list[item[0]][1], keyword)}</a>'\n",
    "            display(HTML(f'[{item[1]}] {clickable_title}'))\n",
    "\n",
    "    def cache_segmented_words(self):\n",
    "        for news in self.news_list:\n",
    "            for word in set(jieba.cut(news[1] + news[2], cut_all=True)):\n",
    "                if word not in self.cache:\n",
    "                    r = self.search_keywords(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         38129453 function calls in 219.694 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      " 27155088  183.466    0.000  183.466    0.000 {method 'lower' of 'str' objects}\n",
      "    14023   24.962    0.002  211.123    0.015 3975915539.py:78(<listcomp>)\n",
      "   159850    3.385    0.000    3.715    0.000 __init__.py:180(get_DAG)\n",
      "  1130100    1.693    0.000    6.074    0.000 __init__.py:198(__cut_all)\n",
      "  1279080    1.646    0.000    1.646    0.000 {method 'count' of 'str' objects}\n",
      "   639540    1.056    0.000   17.189    0.000 3975915539.py:68(score)\n",
      "  1177620    0.919    0.000    7.410    0.000 __init__.py:289(cut)\n",
      "        1    0.719    0.719  219.667  219.667 3975915539.py:93(cache_segmented_words)\n",
      "  1176260    0.558    0.000    0.558    0.000 {method 'match' of 're.Pattern' objects}\n",
      "  2191330    0.265    0.000    0.265    0.000 {method 'append' of 'list' objects}\n",
      "  1726290    0.197    0.000    0.197    0.000 {built-in method builtins.len}\n",
      "   160610    0.177    0.000    0.177    0.000 {method 'split' of 're.Pattern' objects}\n",
      "    14023    0.163    0.000    0.249    0.000 {method 'sort' of 'list' objects}\n",
      "    24888    0.160    0.000  211.538    0.008 3975915539.py:73(search_keywords)\n",
      "   159850    0.107    0.000    0.172    0.000 _compat.py:73(<lambda>)\n",
      "   639540    0.086    0.000    0.086    0.000 3975915539.py:79(<lambda>)\n",
      "   159850    0.042    0.000    0.042    0.000 __init__.py:168(check_initialized)\n",
      "   159850    0.035    0.000    0.035    0.000 {built-in method builtins.iter}\n",
      "   159850    0.030    0.000    0.030    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.026    0.026  219.694  219.694 <string>:1(<module>)\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method _pickle.load}\n",
      "      900    0.001    0.000    0.001    0.000 _compat.py:76(strdecode)\n",
      "      900    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method nt.stat}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
      "        1    0.000    0.000  219.694  219.694 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "        1    0.000    0.000    0.001    0.001 3975915539.py:20(fetch_data)\n",
      "        1    0.000    0.000  219.668  219.668 3975915539.py:13(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 genericpath.py:16(exists)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
     ]
    }
   ],
   "source": [
    "%prun searcherC5 = MySearcherC5(scale=10)\n",
    "\n",
    "# 78是list comprehension 也就是function search_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MySearcherC6V1(MySearcherC5):\n",
    "\n",
    "    \"\"\"\n",
    "    尽量减少lower的运行次数\n",
    "    文本是得lower 但是可以只预处理lower一次 就不要搜一次词lower一次\n",
    "    \"\"\"\n",
    "    def __init__(self, scale=1):\n",
    "        self.news_list = []\n",
    "        self.fetch_data()\n",
    "        self.news_list *= scale\n",
    "        self.cache = {}\n",
    "        self.normalize_case_in_news()\n",
    "        self.cache_segmented_words()\n",
    "        \n",
    "\n",
    "    # 把标题 正文粘在一起 整体来了个lower，把结果放在news list每项最后 1 url 2 标题 3 正文 3 小写化内容\n",
    "    def normalize_case_in_news(self):\n",
    "        for index in range(len(self.news_list)):\n",
    "            self.news_list[index].append((self.news_list[index][1] + self.news_list[index][2]).lower())\n",
    "\n",
    "    def search_keywords(self, keyword):\n",
    "        keyword_lower = keyword.lower()\n",
    "        if keyword_lower in self.cache:\n",
    "            result = self.cache[keyword_lower]\n",
    "        else:\n",
    "            result = [(i, self.score(item, keyword)) for i, item in enumerate(self.news_list) if (keyword_lower in item[3])]\n",
    "            result.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.cache[keyword_lower] = result\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 36.2 s\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcherC6V1 = MySearcherC6V1(scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySearcherC6V2(MySearcherC6V1):\n",
    "\n",
    "    \"\"\"\n",
    "    用文档刷词 构建缓存\n",
    "    \"\"\"\n",
    "    def cache_segmented_words(self):\n",
    "        \n",
    "        for i, news in enumerate(self.news_list):\n",
    "            words = set(word.lower() for word in jieba.cut(news[1] + news[2], cut_all=True))\n",
    "            for word in words:\n",
    "                result_tuple = (i, self.score(news, word))\n",
    "\n",
    "                if word not in self.cache:\n",
    "                    self.cache[word] = [result_tuple]\n",
    "\n",
    "                else:\n",
    "                    self.cache[word].append(result_tuple)\n",
    "\n",
    "        for word in self.cache:\n",
    "            self.cache[word].sort(key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16.3 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcherC6V2 = MySearcherC6V2(scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySearcherC6V3(MySearcherC6V2):\n",
    "    \"\"\"\n",
    "    删掉search keywords里的文档扫描过程\n",
    "    现在有倒排索引啰 我们认为出现在索引的词才有 索引里没出现的词文档里就没有 没词就没必要再遍历、刷所有文档 避免不必要的计算 不然第一个用户倒霉\n",
    "    但是现在我们把所有注押cache上了 该搜到的没搜到就是分词的锅 解决办法 - jieba load userdict\n",
    "    \"\"\"\n",
    "\n",
    "    def search_keywords(self, keyword):\n",
    "        keyword_lower = keyword.lower()\n",
    "        if keyword_lower in self.cache:\n",
    "            result = self.cache[keyword_lower]\n",
    "        else:\n",
    "            result = []\n",
    "\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
